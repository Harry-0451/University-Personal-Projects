# Computational Creativity

## Project Goal

The goal was to create a user interface for two unrelated systems. One system would create an image based on user input, the second would style that image to a particular genre & later during development I attempted to find a method to categorise said image so that it may be titled.

## Files I worked on

* main.py
* assist.py
* style.py
* title_generator.py

## Further Work
* Another button could be added to generate the style to the image separately from generating the image. This would be useful as depending on the hardware  it can take from about six to eleven minutes to just generate the image. The user may like the image and only want to see the distinctive styles applied instead of just overwriting it with another.

* Another change I would make is the limitation of pictures to choose from in style. The categories could be increased, but more specifically the pictures in each category should be vastly expanded and then randomised to allow for a larger variety of styles and nuances to be created in the final image. 

* The current title function doesnâ€™t quite hit the mark and I believe there could be more effective ways of implementing that system. Grammatical errors occur often with this system & something like MLCONJUG3 could be beneficial to restructure titles using machine learning techniques.


## Generated Images

<p align="center">
  
  <img src="Produced Work/Variety Test/Ball in an ocean.png" width="200" />
  <img src="Produced Work/Variety Test/Bike in space.png" width="200" />
  <img src="Produced Work/Variety Test/Cliff on top of a house.png" width="200" />
  <img src="Produced Work/Variety Test/House under the sea.png" width="200" />
  <img src="Produced Work/Variety Test/Man on the moon.png" width="200" />
  
  <img src="Produced Work/Spontaneity & Originality Test/Space in a bottle 2.png" width="200" />
  
  <img src="Produced Work/Persistence Test/500 Iterations/black_hole 5.png" width="200" />
  
</p>

## Program Screenshots

### Main Window
<p align="center">
  <img src="GitHub/Window.png">
</p>


### User Inputs Text Prompt
<p align="center">
  <img src="GitHub/UserInput.png">
</p>

### User Selects Style
<p align="center">
  <img src="GitHub/UserStyle.png">
</p>

### Program Generates Output
<p align="center">
  <img src="GitHub/WindowOutput.png">
</p>

### Program In Use
  
<p align="center">
  <img src="Produced Work/Interaction Test/Juice on a moose.png" width="500" />
  <img src="Produced Work/Interaction Test/house in the clouds.png" width="500" />
  <img src="Produced Work/Interaction Test/Robot flying.png" width="500" />
  <img src="Produced Work/Interaction Test/Wide open field in mid century england.png" width="500" />
  <img src="Produced Work/Interaction Test/Man on pavement.png" width="500" />
</p>

### Notes
* There is a large data set not added to open_images_annotations_100 as the files are too big for GitHub to take. This may cause the program not to work on other computers.

## Citations

Nerdy Rodent- https://github.com/nerdyrodent/VQGAN-CLIP

Katherine Crowson - <https://github.com/crowsonkb>
